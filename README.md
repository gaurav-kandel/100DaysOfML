# 100 Days of Machine Learning

---

**Day 1:** Introduction to Machine Learning  
**Day 2:** Supervised vs Unsupervised Learning  
**Day 3:** Common Types of Machine Learning Algorithms  
**Day 4:** Regression vs Classification Problems  
**Day 5:** Introduction to Linear Regression  
**Day 6:** Assumptions of Linear Regression  
**Day 7:** Simple vs Multiple Linear Regression  
**Day 8:** Gradient Descent in Linear Regression  
**Day 9:** Introduction to Logistic Regression  
**Day 10:** Sigmoid Function and Cost Function in Logistic Regression  

---

**Day 11:** Regularization in Regression (L1/Lasso, L2/Ridge)  
**Day 12:** Elastic Net Regularization  
**Day 13:** Polynomial Regression  
**Day 14:** Feature Engineering in ML  
**Day 15:** One-Hot Encoding & Label Encoding  
**Day 16:** Feature Scaling: Standardization vs Normalization  
**Day 17:** Feature Selection Methods (Filter, Wrapper, Embedded)  
**Day 18:** Introduction to Decision Trees  
**Day 19:** Entropy and Information Gain in Decision Trees  
**Day 20:** Gini Index in Decision Trees  

---

**Day 21:** Overfitting and Underfitting  
**Day 22:** Cross Validation: k-Fold, Stratified k-Fold  
**Day 23:** Hyperparameter Tuning: Grid Search vs Random Search  
**Day 24:** Pruning in Decision Trees  
**Day 25:** Introduction to Support Vector Machines (SVM)  
**Day 26:** SVM Kernel Tricks  
**Day 27:** Introduction to k-Nearest Neighbors (k-NN)  
**Day 28:** Distance Metrics in k-NN  
**Day 29:** Introduction to Naive Bayes Classifier  
**Day 30:** Bayes Theorem in Machine Learning  

---

**Day 31:** Gaussian Naive Bayes vs Multinomial Naive Bayes  
**Day 32:** Introduction to Ensemble Learning  
**Day 33:** Bagging vs Boosting  
**Day 34:** Random Forests  
**Day 35:** Feature Importance in Random Forests  
**Day 36:** Introduction to Gradient Boosting  
**Day 37:** AdaBoost Algorithm  
**Day 38:** XGBoost Algorithm  
**Day 39:** LightGBM Algorithm  
**Day 40:** CatBoost Algorithm  

---

**Day 41:** Introduction to k-Means Clustering  
**Day 42:** Elbow Method for Optimal Clusters  
**Day 43:** Silhouette Score for Cluster Validation  
**Day 44:** Introduction to Hierarchical Clustering  
**Day 45:** Agglomerative vs Divisive Clustering  
**Day 46:** Introduction to DBSCAN  
**Day 47:** Understanding the Curse of Dimensionality  
**Day 48:** Dimensionality Reduction Techniques  
**Day 49:** Principal Component Analysis (PCA)  
**Day 50:** LDA (Linear Discriminant Analysis)  

---

**Day 51:** t-SNE for Visualization  
**Day 52:** Feature Importance Techniques  
**Day 53:** Handling Imbalanced Datasets  
**Day 54:** Oversampling Techniques (e.g., Random Oversampling)  
**Day 55:** SMOTE (Synthetic Minority Oversampling Technique)  
**Day 56:** Undersampling Techniques  
**Day 57:** Cost-Sensitive Learning  
**Day 58:** ROC Curve and AUC Score  
**Day 59:** Precision, Recall, F1-Score  
**Day 60:** Confusion Matrix  

---

**Day 61:** Bias-Variance Tradeoff  
**Day 62:** Handling Missing Data (MCAR, MAR, MNAR)  
**Day 63:** Mean/Median/Mode Imputation  
**Day 64:** Advanced Imputation Techniques (KNN, Iterative)  
**Day 65:** Introduction to Anomaly Detection  
**Day 66:** Isolation Forest Algorithm  
**Day 67:** Local Outlier Factor (LOF)  
**Day 68:** Handling Outliers in Data  
**Day 69:** Introduction to Time Series Analysis  
**Day 70:** Stationarity in Time Series  

---

**Day 71:** ACF and PACF in Time Series  
**Day 72:** ARIMA Model  
**Day 73:** SARIMA Model  
**Day 74:** Time Series Decomposition  
**Day 75:** Introduction to Recommender Systems  
**Day 76:** Collaborative Filtering vs Content-Based Filtering  
**Day 77:** User-Based vs Item-Based Collaborative Filtering  
**Day 78:** Cosine Similarity in Recommender Systems  
**Day 79:** Pearson Correlation in Recommender Systems  
**Day 80:** Matrix Factorization for Recommendations  

---

**Day 81:** Cold Start Problem in Recommender Systems  
**Day 82:** Evaluation Metrics for Recommender Systems  
**Day 83:** Introduction to AutoML  
**Day 84:** Introduction to Model Interpretability  
**Day 85:** SHAP (SHapley Additive exPlanations)  
**Day 86:** LIME (Local Interpretable Model-Agnostic Explanations)  
**Day 87:** Explainability in Black Box Models  
**Day 88:** Model Monitoring and Deployment  
**Day 89:** Introduction to Feature Stores  
**Day 90:** Building a Complete ML Pipeline  

---

**Day 91:** Model Drift and How to Handle It  
**Day 92:** Introduction to Fairness in Machine Learning  
**Day 93:** AI Bias and How to Mitigate It  
**Day 94:** Fairness-Aware Algorithms  
**Day 95:** Interview Preparation: Basic ML Concepts  
**Day 96:** Interview Preparation: ML Algorithms  
**Day 97:** Interview Preparation: Model Evaluation Techniques  
**Day 98:** Interview Preparation: Model Selection and Tuning  
**Day 99:** Interview Preparation: Scenario-Based ML Questions  
**Day 100:** Interview Preparation: Full Mock Interviews  

---
